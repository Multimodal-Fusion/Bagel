{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Bytedance Ltd. and/or its affiliates.\n",
    "# SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import (\n",
    "    Any,\n",
    "    AsyncIterable,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Generator,\n",
    "    List,\n",
    "    NamedTuple,\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from accelerate import infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights\n",
    "\n",
    "from data.transforms import ImageTransform\n",
    "from data.data_utils import pil_img2rgb, add_special_tokens\n",
    "from modeling.bagel import (\n",
    "    BagelConfig, Bagel, Qwen2Config, Qwen2ForCausalLM, SiglipVisionConfig, SiglipVisionModel\n",
    ")\n",
    "from modeling.qwen2 import Qwen2Tokenizer\n",
    "from modeling.bagel.qwen2_navit import NaiveCache\n",
    "from modeling.autoencoder import load_ae\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fusion/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/fusion/lib/python3.10/site-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187d04cb407b4b08ac1575a3447fb11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3187c81d59496ebc7d2e7d86a7ab4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ae.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f272db5ef84da79a5e5538298f6acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ema.safetensors:   0%|          | 0.00/29.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377d89a18d264bcaa32e3812f601ae56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9b2ba3ffd64531b5e59d63f63c3a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09cd17b358e4f489f283cfd8ccd316c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0209331284804bea8b14054f20f6e10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b28ef0b986a43cb8acb64331eae2cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8409dec50f43a2ad884b1906ffc3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ccecad91e847178cf1b4dcac2b0c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec792b42f3a4cc3920bdbf68c5fd23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vit_config.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f077844f0446dab00e535b819fcbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7c2d49b0404763a605863d253bf05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/localssd/models/BAGEL-7B-MoT'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "save_dir = \"./models/BAGEL-7B-MoT\"\n",
    "repo_id = \"ByteDance-Seed/BAGEL-7B-MoT\"\n",
    "cache_dir = save_dir + \"/cache\"\n",
    "\n",
    "snapshot_download(cache_dir=cache_dir,\n",
    "  local_dir=save_dir,\n",
    "  repo_id=repo_id,\n",
    "  local_dir_use_symlinks=False,\n",
    "  resume_download=True,\n",
    "  allow_patterns=[\"*.json\", \"*.safetensors\", \"*.bin\", \"*.py\", \"*.md\", \"*.txt\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"./models/BAGEL-7B-MoT/\"  # Download from https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT\n",
    "\n",
    "# LLM config preparing\n",
    "llm_config = Qwen2Config.from_json_file(os.path.join(model_path, \"llm_config.json\"))\n",
    "llm_config.qk_norm = True\n",
    "llm_config.tie_word_embeddings = False\n",
    "llm_config.layer_module = \"Qwen2MoTDecoderLayer\"\n",
    "\n",
    "# ViT config preparing\n",
    "vit_config = SiglipVisionConfig.from_json_file(os.path.join(model_path, \"vit_config.json\"))\n",
    "vit_config.rope = False\n",
    "vit_config.num_hidden_layers = vit_config.num_hidden_layers - 1\n",
    "\n",
    "# VAE loading\n",
    "vae_model, vae_config = load_ae(local_path=os.path.join(model_path, \"ae.safetensors\"))\n",
    "\n",
    "# Bagel config preparing\n",
    "config = BagelConfig(\n",
    "    visual_gen=True,\n",
    "    visual_und=True,\n",
    "    llm_config=llm_config, \n",
    "    vit_config=vit_config,\n",
    "    vae_config=vae_config,\n",
    "    vit_max_num_patch_per_side=70,\n",
    "    connector_act='gelu_pytorch_tanh',\n",
    "    latent_patch_size=2,\n",
    "    max_latent_size=64,\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    language_model = Qwen2ForCausalLM(llm_config)\n",
    "    vit_model      = SiglipVisionModel(vit_config)\n",
    "    model          = Bagel(language_model, vit_model, config)\n",
    "    model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)\n",
    "\n",
    "# Tokenizer Preparing\n",
    "tokenizer = Qwen2Tokenizer.from_pretrained(model_path)\n",
    "tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)\n",
    "\n",
    "# Image Transform Preparing\n",
    "vae_transform = ImageTransform(1024, 512, 16)\n",
    "vit_transform = ImageTransform(980, 224, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading and Multi GPU Infernece Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_mem_per_gpu = \"40GiB\"  # Modify it according to your GPU setting. On an A100, 80 GiB is sufficient to load on a single GPU.\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    model,\n",
    "    max_memory={i: max_mem_per_gpu for i in range(torch.cuda.device_count())},\n",
    "    no_split_module_classes=[\"Bagel\", \"Qwen2MoTDecoderLayer\"],\n",
    ")\n",
    "print(device_map)\n",
    "\n",
    "same_device_modules = [\n",
    "    'language_model.model.embed_tokens',\n",
    "    'time_embedder',\n",
    "    'latent_pos_embed',\n",
    "    'vae2llm',\n",
    "    'llm2vae',\n",
    "    'connector',\n",
    "    'vit_pos_embed'\n",
    "]\n",
    "\n",
    "if torch.cuda.device_count() == 1:\n",
    "    first_device = device_map.get(same_device_modules[0], \"cuda:0\")\n",
    "    for k in same_device_modules:\n",
    "        if k in device_map:\n",
    "            device_map[k] = first_device\n",
    "        else:\n",
    "            device_map[k] = \"cuda:0\"\n",
    "else:\n",
    "    first_device = device_map.get(same_device_modules[0])\n",
    "    for k in same_device_modules:\n",
    "        if k in device_map:\n",
    "            device_map[k] = first_device\n",
    "\n",
    "# Thanks @onion-liu: https://github.com/ByteDance-Seed/Bagel/pull/8\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    checkpoint=os.path.join(model_path, \"ema.safetensors\"),\n",
    "    device_map=device_map,\n",
    "    offload_buffers=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    force_hooks=True,\n",
    "    offload_folder=\"/tmp/offload\"\n",
    ")\n",
    "\n",
    "model = model.eval()\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencer Preparing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from inferencer import InterleaveInferencer\n",
    "\n",
    "inferencer = InterleaveInferencer(\n",
    "    model=model, \n",
    "    vae_model=vae_model, \n",
    "    tokenizer=tokenizer, \n",
    "    vae_transform=vae_transform, \n",
    "    vit_transform=vit_transform, \n",
    "    new_token_ids=new_token_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About Inference Hyperparameters:**\n",
    "- **`cfg_text_scale`:** Controls how strongly the model follows the text prompt. `1.0` disables text guidance. Typical range: `4.0–8.0`.\n",
    "- **`cfg_image_scale`:** Controls how much the model preserves input image details. `1.0` disables image guidance. Typical range: `1.0–2.0`.\n",
    "- **`cfg_interval`:** Fraction of denoising steps where CFG is applied. Later steps can skip CFG to reduce computation. Typical: `[0.4, 1.0]`.\n",
    "- **`timestep_shift`:** Shifts the distribution of denoising steps. Higher values allocate more steps at the start (affects layout); lower values allocate more at the end (improves details).\n",
    "- **`num_timesteps`:** Total denoising steps. Typical: `50`.\n",
    "- **`cfg_renorm_min`:** Minimum value for CFG-Renorm. `1.0` disables renorm. Typical: `0`.\n",
    "- **`cfg_renorm_type`:** CFG-Renorm method:  \n",
    "  - `global`: Normalize over all tokens and channels (default for T2I).\n",
    "  - `channel`: Normalize across channels for each token.\n",
    "  - `text_channel`: Like `channel`, but only applies to text condition (good for editing, may cause blur).\n",
    "- **If edited images appear blurry, try `global` CFG-Renorm, decrease `cfg_renorm_min` or decrease `cfg_scale`.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Bagel - basic capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_hyper=dict(\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=1.0,\n",
    "    cfg_interval=[0.4, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=50,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"global\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"A female cosplayer portraying an ethereal fairy or elf, wearing a flowing dress made of delicate fabrics in soft, mystical colors like emerald green and silver. She has pointed ears, a gentle, enchanting expression, and her outfit is adorned with sparkling jewels and intricate patterns. The background is a magical forest with glowing plants, mystical creatures, and a serene atmosphere.\"\n",
    "\n",
    "print(prompt)\n",
    "print('-' * 10)\n",
    "output_dict = inferencer(text=prompt, **inference_hyper)\n",
    "display(output_dict['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Image Generation with Think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=1.0,\n",
    "    cfg_interval=[0.4, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=50,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"global\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = 'a car made of small cars'\n",
    "\n",
    "print(prompt)\n",
    "print('-' * 10)\n",
    "output_dict = inferencer(text=prompt, think=True, **inference_hyper)\n",
    "print(output_dict['text'])\n",
    "display(output_dict['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_hyper=dict(\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=50,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = Image.open('test_images/women.jpg')\n",
    "prompt = 'She boards a modern subway, quietly reading a folded newspaper, wearing the same clothes.'\n",
    "\n",
    "display(image)\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "output_dict = inferencer(image=image, text=prompt, **inference_hyper)\n",
    "display(output_dict['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit with Think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=50,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = Image.open('test_images/octupusy.jpg')\n",
    "prompt = 'Could you display the sculpture that takes after this design?'\n",
    "\n",
    "display(image)\n",
    "print('-'*10)\n",
    "output_dict = inferencer(image=image, text=prompt, think=True, **inference_hyper)\n",
    "print(output_dict['text'])\n",
    "display(output_dict['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"Can someone explain what’s funny about this meme??\"\n",
    "\n",
    "display(image)\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "output_dict = inferencer(image=image, text=prompt, understanding_output=True, **inference_hyper)\n",
    "print(output_dict['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Exploring interleaved capabilties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Basic non-reasoning / reasoning inference with text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is 2 + 2 x 4\n",
      "----------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inferencer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[43minferencer\u001b[49m(text\u001b[38;5;241m=\u001b[39mprompt, understanding_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minference_hyper)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inferencer' is not defined"
     ]
    }
   ],
   "source": [
    "# Basic text outputs\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    ")\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"what is 2 + 2 x 4\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "output_dict = inferencer(text=prompt, understanding_output=True, **inference_hyper)\n",
    "print(output_dict['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text outputs + reasoning\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    ")\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"what is 2 + 2 x 4\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "print(output_dict['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Interleaved Inputs - Single Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 - Text output - Multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Basic text outputs + reasoning\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"what is 2 + 2 x 4\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Text output - two images\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"can you help me tell what the difference between the two images is?\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [image1, image2, prompt]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Text output - three images\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"can you help me tell what the difference between the two images is?\"\n",
    "prompt = \"which image first or third is best match for the second image? and why?\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [image1, image2, image3, prompt]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Text output - three images - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"can you help me tell what the difference between the two images is?\"\n",
    "prompt = \"which image first or third is best match for the second image? and why?\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [image1, image2, image3, prompt]\n",
    "input_list = [prompt, image2, image1, image3]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Text output - three images - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"can you help me tell what the difference between the two images is?\"\n",
    "prompt = \"which image first or third is best match for the second image? and why?\"\n",
    "prompt = \"Given above reference image, which image first or second is most similar to the reference? and why?\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [image1, image2, image3, prompt]\n",
    "input_list = [image2, prompt, image1, image3]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Text output - three images - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"can you help me tell what the difference between the two images is?\"\n",
    "prompt = \"which image first or third is best match for the second image? and why?\"\n",
    "prompt = \"Given above reference image, which image first or second is most similar to the reference? and why?\"\n",
    "prompt = \"If these images were presented in a sequential order in a video, what would be a nice detailed description for what happend in the three frames?\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [image1, image2, image3, prompt]\n",
    "input_list = [image2, prompt, image1, image3]\n",
    "input_list = [prompt, image2, image1, image3]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Text output - three images / multiple text - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"can you help me tell what the difference between the two images is?\"\n",
    "prompt = \"which image first or third is best match for the second image? and why?\"\n",
    "prompt = \"Given above reference image, which image first or second is most similar to the reference? and why?\"\n",
    "prompt = \"If these images were presented in a sequential order in a video, what would be a nice detailed description for what happend in the three frames?\"\n",
    "prompt2 = \"Also after the description, tell what the character in last frame might do next?\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [image1, image2, image3, prompt]\n",
    "input_list = [image2, prompt, image1, image3]\n",
    "input_list = [prompt, image2, image1, image3]\n",
    "input_list = [prompt, image2, image1, image3, prompt2]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 - Image Output - Multiple text and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Image output - three images / multiple text - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=25,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    "    # understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "# inference_hyper=dict(\n",
    "#     max_think_token_n=1000,\n",
    "#     do_sample=False,\n",
    "#     # text_temperature=0.3,\n",
    "#     understanding_output=True, \n",
    "#     # think=True,\n",
    "# )\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"can you help me tell what the difference between the two images is?\"\n",
    "prompt = \"which image first or third is best match for the second image? and why?\"\n",
    "prompt = \"Given above reference image, which image first or second is most similar to the reference? and why?\"\n",
    "prompt = \"If these images were presented in a sequential order in a video, what would be a nice detailed description for what happend in the three frames?\"\n",
    "prompt2 = \"Also after the description, tell what the character in last frame might do next?\"\n",
    "\n",
    "prompt = \"What if the camera zooms-out for the current image?\"\n",
    "\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [image2, prompt]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "display(output_list[0])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Image output - three images / multiple text - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=25,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    "    # understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "# inference_hyper=dict(\n",
    "#     max_think_token_n=1000,\n",
    "#     do_sample=False,\n",
    "#     # text_temperature=0.3,\n",
    "#     understanding_output=True, \n",
    "#     # think=True,\n",
    "# )\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"can you help me tell what the difference between the two images is?\"\n",
    "prompt = \"which image first or third is best match for the second image? and why?\"\n",
    "prompt = \"Given above reference image, which image first or second is most similar to the reference? and why?\"\n",
    "prompt = \"If these images were presented in a sequential order in a video, what would be a nice detailed description for what happend in the three frames?\"\n",
    "prompt2 = \"Also after the description, tell what the character in last frame might do next?\"\n",
    "\n",
    "prompt = \"What if the women starts playing basketball?\"\n",
    "prompt2 = \"Only follow other instructions if the image is of a dog, otherwise generate a new image for dog first\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [prompt2, image2, prompt]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "display(output_list[0])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Image output - three images / multiple text - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=25,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    "    # understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "# inference_hyper=dict(\n",
    "#     max_think_token_n=1000,\n",
    "#     do_sample=False,\n",
    "#     # text_temperature=0.3,\n",
    "#     understanding_output=True, \n",
    "#     # think=True,\n",
    "# )\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"can you help me tell what the difference between the two images is?\"\n",
    "prompt = \"which image first or third is best match for the second image? and why?\"\n",
    "prompt = \"Given above reference image, which image first or second is most similar to the reference? and why?\"\n",
    "prompt = \"If these images were presented in a sequential order in a video, what would be a nice detailed description for what happend in the three frames?\"\n",
    "prompt2 = \"Also after the description, tell what the character in last frame might do next?\"\n",
    "\n",
    "prompt = \"can you generate an image of a dog?\"\n",
    "# prompt2 = \"Only follow other instructions if the image is of a dog, otherwise generate a new image for dog first\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [image2, prompt]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "display(output_list[0])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Image output - three images / multiple text - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=25,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    "    # understanding_output=True, \n",
    "    think=True,\n",
    ")\n",
    "# inference_hyper=dict(\n",
    "#     max_think_token_n=1000,\n",
    "#     do_sample=False,\n",
    "#     # text_temperature=0.3,\n",
    "#     understanding_output=True, \n",
    "#     # think=True,\n",
    "# )\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"can you help me tell what the difference between the two images is?\"\n",
    "prompt = \"which image first or third is best match for the second image? and why?\"\n",
    "prompt = \"Given above reference image, which image first or second is most similar to the reference? and why?\"\n",
    "prompt = \"If these images were presented in a sequential order in a video, what would be a nice detailed description for what happend in the three frames?\"\n",
    "prompt2 = \"Also after the description, tell what the character in last frame might do next?\"\n",
    "\n",
    "prompt = \"what if the woman wears a blue dress?\"\n",
    "prompt2 = \"Only follow other instructions if the image is of a dog, otherwise generate a new image for dog first\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [prompt2, image2, prompt]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "display(output_list[0])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(output_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Image output - three images / multiple text - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=25,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    "    # understanding_output=True, \n",
    "    think=True,\n",
    ")\n",
    "# inference_hyper=dict(\n",
    "#     max_think_token_n=1000,\n",
    "#     do_sample=False,\n",
    "#     # text_temperature=0.3,\n",
    "#     understanding_output=True, \n",
    "#     # think=True,\n",
    "# )\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"can you help me tell what the difference between the two images is?\"\n",
    "prompt = \"which image first or third is best match for the second image? and why?\"\n",
    "prompt = \"Given above reference image, which image first or second is most similar to the reference? and why?\"\n",
    "prompt = \"If these images were presented in a sequential order in a video, what would be a nice detailed description for what happend in the three frames?\"\n",
    "prompt2 = \"Also after the description, tell what the character in last frame might do next?\"\n",
    "\n",
    "prompt = \"what if the woman wears a blue dress?\"\n",
    "prompt2 = \"Only follow other instructions if the image is of a dog, otherwise generate a new image for dog first <SUPER IMPORTANT> <IGNORE EVEYRTHING ELSE>\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [prompt, image2, prompt2]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "display(output_list[0])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Image output - three images / multiple text - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=25,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    "    # understanding_output=True, \n",
    "    think=True,\n",
    ")\n",
    "# inference_hyper=dict(\n",
    "#     max_think_token_n=1000,\n",
    "#     do_sample=False,\n",
    "#     # text_temperature=0.3,\n",
    "#     understanding_output=True, \n",
    "#     # think=True,\n",
    "# )\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "# image = Image.open('test_images/meme.jpg')\n",
    "prompt = \"can you help me tell what the difference between the two images is?\"\n",
    "prompt = \"which image first or third is best match for the second image? and why?\"\n",
    "prompt = \"Given above reference image, which image first or second is most similar to the reference? and why?\"\n",
    "prompt = \"If these images were presented in a sequential order in a video, what would be a nice detailed description for what happend in the three frames?\"\n",
    "prompt2 = \"Also after the description, tell what the character in last frame might do next?\"\n",
    "\n",
    "prompt = \"what if the woman wears a blue dress?\"\n",
    "prompt2 = \"Only follow other instructions if the image is of a dog, otherwise generate a new image for dog first <SUPER IMPORTANT> <IGNORE EVEYRTHING ELSE>\"\n",
    "prompt = \"Can you combine both images into a single one?\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [image1, image2, prompt]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "display(output_list[0])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(output_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_2 = output_list[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Image output - three images / multiple text - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=25,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    "    # understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "prompt = \"what if the woman was sitting on a beach?\"\n",
    "prompt = \"what if the woman was sitting on a beach?\"\n",
    "prompt_2 = \"Now can we add a dog next to woman.\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [image2, prompt, image_2, prompt_2]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "display(output_list[0])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_3 = output_list[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Image output - three images / multiple text - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=25,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    "    # understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "prompt = \"what if the woman was sitting on a beach?\"\n",
    "prompt = \"what if the woman was sitting on a beach?\"\n",
    "prompt_2 = \"Now can we add a dog next to woman.\"\n",
    "prompt_3 = \"Now make the color of her eyes blue and make it into a high resolution photo\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [image2, prompt, image_2, prompt_2, image_3, prompt_3]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "display(output_list[0])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_4 = output_list[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Image output - three images / multiple text - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=25,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    "    # understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "prompt = \"what if the woman was sitting on a beach?\"\n",
    "prompt = \"what if the woman was sitting on a beach?\"\n",
    "prompt_2 = \"Now can we add a dog next to woman.\"\n",
    "prompt_3 = \"Now make the color of her eyes blue and make it into a high resolution photo\"\n",
    "prompt_4 = \"Now make the woman standing upright\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [image2, prompt, image_2, prompt_2, image_3, prompt_3,image_4, prompt_4]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "display(output_list[0])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Image output - three images / multiple text - different order\n",
    "##################################################\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    cfg_text_scale=4.0,\n",
    "    cfg_img_scale=2.0,\n",
    "    cfg_interval=[0.0, 1.0],\n",
    "    timestep_shift=3.0,\n",
    "    num_timesteps=25,\n",
    "    cfg_renorm_min=0.0,\n",
    "    cfg_renorm_type=\"text_channel\",\n",
    "    understanding_output=True, \n",
    "    # think=True,\n",
    ")\n",
    "inference_hyper=dict(\n",
    "    max_think_token_n=1000,\n",
    "    do_sample=False,\n",
    "    # text_temperature=0.3,\n",
    "    understanding_output=True, \n",
    "    think=True,\n",
    ")\n",
    "\n",
    "image1 = Image.open('test_images/octupusy.jpg')\n",
    "image2 = Image.open('test_images/women.jpg')\n",
    "image3 = Image.open('test_images/women.jpg')\n",
    "\n",
    "prompt = \"what if the woman was sitting on a beach?\"\n",
    "prompt = \"what if the woman was sitting on a beach?\"\n",
    "prompt_2 = \"Now can we add a dog next to woman.\"\n",
    "prompt_3 = \"Now make the color of her eyes blue and make it into a high resolution photo\"\n",
    "prompt_4 = \"Now make the woman standing upright\"\n",
    "prompt_5 = \"How many images can you see? and what would be an appropirate description of edit from first image to last? Also what was the last edit instruction? and was it ever applied (be very careful) ... is the woman standing upright?\"\n",
    "\n",
    "prompt_4 = \"Now remove the woman\"\n",
    "prompt_5 = \"Describe in detail the final image. Provide a detailed caption\"\n",
    "prompt_6 = \"provide a detailed description for the video if the provided sequence of images was a video\"\n",
    "\n",
    "print(prompt)\n",
    "print('-'*10)\n",
    "input_list = [prompt]\n",
    "input_list = [image2, prompt, image_2, prompt_2, image_3, prompt_3,image_4, prompt_4, prompt_5]\n",
    "input_list = [image2,image_2, image_3,image_4, prompt_5]\n",
    "input_list = [image2,image_2, image_3,image_4, prompt_4]\n",
    "input_list = [image2,image_2, image_3,image_4, prompt_6]\n",
    "\n",
    "output_list = inferencer.interleave_inference(input_list, **inference_hyper)\n",
    "# output_dict = inferencer(text=prompt, understanding_output=True, think=True, **inference_hyper)\n",
    "# print(output_dict['text'])\n",
    "display(output_list[0])\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "1bfaa82d-51b0-4c13-9e4c-295ba28bcd8a",
  "filePath": "/mnt/bn/seed-aws-va/chaorui/code/cdt-hf/notebooks/chat.ipynb",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
